# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JIWnJmM99vwtzMk4-MQ1ZlAzdVSIVNY4

# **Twitter Sentiment Analysis**


Machine Learning Lab Project 2022-23

---


# Imports and setup
"""

# Python Imports
import datetime, os
from collections import defaultdict

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC

from nltk.corpus import stopwords

from textblob import TextBlob 

from sklearn.decomposition import PCA, TruncatedSVD, SparsePCA
from sklearn.preprocessing import FunctionTransformer
from sklearn.feature_extraction.text import CountVectorizer

import nltk
nltk.download('stopwords')

!pip install profanity

from profanity import profanity

"""# Load Data"""

from google.colab import drive
drive.mount('/content/drive')

train_file = "/content/drive/MyDrive/Fourth Year/Semester 7/Machine Learning/Sentiment Analysis Project/data/train.tsv"
dev_file = "/content/drive/MyDrive/Fourth Year/Semester 7/Machine Learning/Sentiment Analysis Project/data/dev.tsv"
test_file = "/content/drive/MyDrive/Fourth Year/Semester 7/Machine Learning/Sentiment Analysis Project/data/test.tsv"

df_train = pd.read_csv(train_file, delimiter="\t")
df_dev = pd.read_csv(dev_file, delimiter="\t")
df_test = pd.read_csv(test_file, delimiter="\t")

trainX = df_train['text']
devX = df_dev['text']
testX = df_test['text']

trainY = df_train['label']
devY = df_dev['label']

"""# Data Analysis"""

df_train.head()

df_train.describe(), df_dev.describe(), df_test.describe()

"""# Preprocessing"""

#@title Create custom stopwords set

# Remove some stopwords
not_stop_words = set()
not_stop_words.update(['i', 'me', 'we', 'our', 'you', 'your', 'him', 'her', 'his', 'he', 'she', 'their', 'them'])
not_stop_words.update(['against', 'other', 'than', "don't", 'should', "couldn't", "shouldn't"])

stop_words = set(stopwords.words('english'))
stop_words = stop_words - not_stop_words

# # Add additional stopwords
# additional_stop_words = set()
# additional_stop_words.update(['@USER'])
# stop_words.update(add_stop_words)

#@title Explore BOW representation
from sklearn.feature_extraction.text import CountVectorizer

count_vect = CountVectorizer(stop_words=stop_words)
X_train_counts = count_vect.fit_transform(trainX)
X_train_counts.shape

#@title Explore TF-IDF BOW representation
from sklearn.feature_extraction.text import TfidfTransformer

X_train_tfidf = TfidfTransformer(use_idf=True).fit_transform(X_train_counts) # Also try with use_idf = False
X_train_tfidf.shape

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

count_vect = CountVectorizer(stop_words=stop_words)

X_train_counts = count_vect.fit_transform(trainX)
X_train_tfidf = TfidfTransformer(use_idf=True).fit_transform(X_train_counts) # Also try with use_idf = False

X_dev_counts = count_vect.transform(devX)
X_dev_tfidf = TfidfTransformer(use_idf=True).fit_transform(X_dev_counts)

# X_test_counts = count_vect.trc@0512ansform(testX)
# X_test_tfidf = TfidfTransformer(use_idf=True).fit_transform(X_test_counts)

sentiment_transformer = FunctionTransformer(lambda tweets: np.array([TextBlob(tweet).sentiment.polarity + 1 for tweet in tweets]), validate=False)

profanity_transformer = FunctionTransformer(lambda tweets: np.array([profanity.contains_profanity(tweet) for tweet in tweets]), validate=False)

sentiment_trainX = sentiment_transformer.transform(trainX)
sentiment_devX = sentiment_transformer.transform(devX)

profanity_trainX = profanity_transformer.transform(trainX)
profanity_devX = profanity_transformer.transform(devX)

from scipy import sparse

trainX_wsenti = sparse.hstack([X_train_tfidf, sentiment_trainX.reshape(-1, 1), profanity_trainX.reshape(-1, 1)])
devX_wsenti = sparse.hstack([X_dev_tfidf, sentiment_devX.reshape(-1, 1), profanity_devX.reshape(-1, 1)])

trainX_wsenti.shape

trainX_wsenti.data.shape

"""# Model Configuration"""

from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC

clf_multinomialNB = Pipeline([
     ('clf', MultinomialNB()),
])

clf_SGD = Pipeline([
     ('clf', SGDClassifier()),
])

clf_SVC = Pipeline([
     ('clf', SVC(gamma='scale')),
])

clf_ada = Pipeline([
     ('clf', AdaBoostClassifier()),
])

model_configs = {
    'Multinomial Naive Bayes': clf_multinomialNB,
    'SVC with SGD Optimizer': clf_SGD,
    'Support Vector Classifier (SVC)': clf_SVC,
    'Adaboost Classifier': clf_ada,
}

"""# Train Models"""

for model in model_configs.values():
    model.fit(trainX_wsenti, trainY)

"""# Results"""

from sklearn import metrics

results_dict  = defaultdict(defaultdict)

#@title Accuracy
for model_name, model in model_configs.items():
    results_dict[model_name]['dev_accuracy'] = metrics.accuracy_score(devY, model.predict(devX_wsenti))

#@title F1-Score
for model_name, model in model_configs.items():
    results_dict[model_name]['dev_f1_score'] = metrics.f1_score(devY, model.predict(devX_wsenti), average='weighted')

#@title AUC-Score
from sklearn import metrics
for model_name, model in model_configs.items():
    if "Naive Bayes" in model_name:
        results_dict[model_name]['dev_auc'] = metrics.roc_auc_score(devY, model.predict_proba(devX_wsenti)[:,1], average="weighted")
    else:
        results_dict[model_name]['dev_auc'] = metrics.roc_auc_score(devY, model.decision_function(devX_wsenti), average="weighted")

for model_name, model in model_configs.items(): 
  model.predict(testX)

#@title Results Summary
pd.DataFrame.from_dict(results_dict, orient='index').sort_values(by="dev_accuracy", ascending=False)

results_dict

#@title Visualisation of the Result Dictionary for Dev Data
sns.set(rc={"figure.figsize":(12,8)})

df = pd.DataFrame(results_dict)
g = sns.heatmap(df, cmap="Blues", annot=True, fmt='g')
g.get_figure().savefig("heatmap.png")

